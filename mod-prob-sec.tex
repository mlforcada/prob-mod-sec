\input{header.tex}

\begin{document}

%-----------------------------------------------------------------------
\begin{frame}
  \titlepage
\end{frame}

%-----------------------------------------------------------------------
%--------------------------------------------------------------------
\begin{frame}
  \frametitle{Índice}
  \tableofcontents 
\end{frame}

%--------------------------------------------------------------------
\AtBeginSection[]{
  \begin{frame}
    \frametitle{Índice}
    \tableofcontents[currentsection]
  \end{frame}
}
  
\section{Introducción}
\begin{frame} 
\frametitle{Introducción}

\begin{block}{Objetivo}
  Aprender conceptos básicos sobre los modelos probabilísticos de secuencias.
\end{block}

\begin{itemize}
\item Algunas partes se amplían en materiales disponibles en este mismo repositorio, los cuales se citan en la transparencia correspondiente.
\item \textbf{Importante:} Este bloque asume que el alumnado controla conceptos básicos de probabilidad. Si crees que debes aprender (o volver a aprender) estos conceptos, hazlo por favor antes de abordar este bloque. Un buen recurso son las secciones §1.1, §1.2.1–§1.2.5 de la guía \emph{The Linguist's Guide to Statistics} de Birgitte Krenn y Christer Samuelson (\url{http://nlp.stanford.edu/fsnlp/dontpanic.pdf})

\end{itemize}
\end{frame}
\section{Aplicaciones}
\begin{frame}
  \frametitle{Aplicaciones de los modelos probabilisticos de secuencias/1}

Los modelos probabilísticos de secuencias tienen aplicaciones en muchos ámbitos de la informática.

Pero muy particularmente en las aplicaciones que tratan con el
lenguaje humano, en las que las secuencias son \emph{secuencias de
  palabras} o de elementos relacionados con ellas:
\begin{itemize}
\item En \emph{traducción automática estadística} los modelos se usan para decidir entre varias hipótesis de traducción.
\item En \emph{reconocimiento automático del habla} los modelos se usan para decidir entre varias interpretaciones de una alocución.
\item Cuando se hace el \emph{análisis morfológico} de las palabras de
  un texto, algunas tienen más de un posible análisis; los modelos se
  usan para elegir el análisis más verosímil.
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aplicaciones de los modelos probabilisticos de secuencias/2}

Podemos distinguir dos grandes grupos de modelos:
\begin{itemize}
\item modelos que \emph{puntúan} o \emph{ordenan} hipótesis posibles (por ejemplo, traducciones alternativas de una oración en traducción automática estadística):
  \begin{itemize}
  \item En inglés, ¿\emph{tomar una decisión} se dice \emph{take a decision} o \emph{make a decision}?
  \end{itemize}
\item modelos que \emph{explican} o \emph{clasifican} los elementos de una secuencia en términos de otra secuencia (por ejemplo, desambiguadores categoriales o \emph{part-of-speech taggers}):
  \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}}
\emph{I} 	&\emph{saw} 	&\emph{the} 	&\emph{book}\\
Pronombre	&¿verbo o sustantivo? &determinante &¿verbo o substantivo?\\
   \end{tabular}
\end{itemize}

\end{frame}
\section{Preliminares}
\begin{frame}
  \frametitle{Generalidades}
  \begin{itemize}
  \item Los modelos probabilísticos de secuencias asignan probabilidades a las secuencias. Por ejemplo:
    \begin{itemize}
    \item Imaginemos un  vocabulario \(V\) tiene 2 palabras \(V=\{a,b\}\).
    \item Llamaremos
      \(V^\star=\{\epsilon,a,b,aa,ab,ba,bb,aaa,aab,\ldots\}\)
      al conjunto (infinito) de todas las secuencias finitas que se
      pueden formar con palabras de \(V\)\footnote{El conjunto tiene el mismo tamaño que el de los números naturales}  \(\epsilon\) es la secuencia de cero palabras).
    \item Así, por ejemplo, la probabilidad de \emph{aba}, \(p(aba)\)
      podría ser 0,00036.
    \item La suma de las probabilidades de todas las secuencias debe ser 1:
\[p(\epsilon)+p(a)+p(b)+p(aa)+p(ab)+p(ba)+\ldots = 1\]
    \end{itemize}
  \end{itemize}

\end{frame}


\begin{frame}
\frametitle{Dos tipos de modelos}

En este bloque estudiaremos dos tipos de modelos:
  \begin{itemize}
  \item  Los modelos de \(n\)-gramas, y
  \item los modelos ocultos de Markov.
  \end{itemize}

\end{frame}

\section{Descomposición en producto}
\begin{frame}
  \frametitle{Aproximación factible de \(p(\cdot)\)/1}
  \begin{itemize}
  \item Cómo hacemos que una funcion como \(p(\cdot)\), que toma valores de un conjunto infinito, sea computacionalmente factible?
  \item No podemos mantener una tabla infinita!
  \end{itemize}


\end{frame}

\begin{frame}
  \frametitle{Aproximación factible de \(p(\cdot)\)/2}
  Descomposición en producto:
  

  \begin{itemize}
  \item Las probabilidades de secuencias se pueden rescribir como productos de probabilidades condicionales:
\[p(abaab)=p(a)\times p(b|a) \times p(a|ab) \times p(a|aba) \times p(b|abaa) \times p(\$|abaab)\]
donde \(\$\) es un indicador de final de secuencia.
  \item Sin embargo, el modelo sigue sin ser factible. El segundo argumento de cada factor \(p(\cdot|\cdot)\) toma valores de un conjunto infinito.
  \end{itemize}
\end{frame}

\section{Modelos de \(n\)-gramas}
\begin{frame}
  \frametitle{Aproximación de memoria finita: modelos de \(n\)-gramas /2}
  \begin{itemize}
  \item El séptimo término de \(p(abbabbabba)\) sería
\[p(b|abbabba)\]. 
  \item Podemos aproximarlo \emph{olvidando} palabras antiguas y haciendo que sólo dependa, por ejemplo de las 2 últimas:
  \[p(b|abbabba)\simeq p(b|ba)\]
  Ahora sólo hay tres palabras involucradas (la actual y dos de historia).
  \item Esto sería un modelo de \emph{trigramas} (3-gramas):
\[p(abaab) \simeq p(a)\times p(b|a) \times p(a|ab) \times p(a|ba) \times p(b|aa) \times p(\$|ab)\]

  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aproximación de memoria finita: modelos de \(n\)-gramas /2}
  \begin{itemize}
  \item Los modelos de \(n\)-gramas son factibles, pero aún son muy grandes:
    \begin{itemize}
    \item Imaginemos un vocabulario \(V\) de 10.000 palabras: \(|V|=10000\)
    \item El modelo debería almacenar los \(|V|^3=10^{12}\) trigramas!
    \item En la práctica sólo se almacenan los trigramas vistos en un corpus de entrenamiento (véase más abajo).
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Aproximación de memoria finita: modelos de \(n\)-gramas /3} 
\begin{itemize}
\item Por ejemplo:
\[
\begin{array}{rcl}
 p(\mbox{I saw the book at home})  &=& p(\mbox{I})\times p(\mbox{saw}|\mbox{I}) \times \\
 && \times p(\mbox{the}|\mbox{I saw}) \times  p(\mbox{book}|\mbox{saw the}) \times 
\\
 && \times p(\mbox{at}|\mbox{the book}) \times p(\mbox{at}|\mbox{the book}) \times \\
 && \times  p(\mbox{home}|\mbox{book at}) \times p(\$|\mbox{at home}).
\end{array}
\]
\item El cálculo de \(p(\mbox{saw I at home the book})\) daría una probabilidad más baja: la  oración es menos gramatical que la anterior.
\item Otro ejemplo: en un sistema de reconocimiento automático del habla, el modelo asignaría una probabilidad más alta a \emph{sex and violence on TV} que a \emph{sax and violins on TV} (que suena extremadamente parecida).


\end{itemize}
  \begin{block}{Tarea}
    Estudiar hasta la sección \emph{El modelo de \(k\)-gramas}
    del documento auxiliar \emph{Introducción a los modelos
      probabilísticos de secuencias}, \url{mod-prob-sec-n1.pdf}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Aproximación de memoria finita: modelos de \(n\)-gramas /4} 
De dónde se sacan estas probabilidades?
\begin{itemize}
\item Se aproximan por las frecuencias relativas observadas en el corpus (esto se llama \emph{verosimilitud máxima}.
\item Esto exige \emph{contar} los \(n\)-gramas.
\item Por ejemplo, 
\[
p(\mbox{book}|\mbox{saw the})=\frac{n(\mbox{saw the book})}{n(\mbox{saw the book}) +n(\mbox{saw the house}) + \ldots }
\]
(en el denominador estarían todos los trigramas \emph{saw the \(X\)}, donde \(X\) es cualquier palabra del vocabulario).

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Aproximación de memoria finita: modelos de \(n\)-gramas /5} 
El problema de los eventos no vistos:
\begin{itemize}
\item Si la probabilidad de una oración se calcula a partir de frecuencias relativas\ldots
\item \ldots puede ocurrir que contenga un trigrama no visto.
\item Un trigrama no visto, por ejemplo \emph{book at home} tiene \(n(\mbox{book at home})=0\).
\item Esto significa que \(p(\mbox{home}|\mbox{book at})\) es cero, y por tanto, toda la secuencia de productos de la oración \emph{I saw the book at home} es nula.
\item Se debe buscar alguna manera de asignar una probabilidad no nula a los trigramas no vistos: esto se denomina \emph{suavizado}.
  \begin{itemize}
  \item Se trata de quitar un poco de probabilidad a los trigramas vistos y repartirlo entre los no vistos.
  \end{itemize}
\end{itemize}
  \begin{block}{Tarea}
    Estudiar las secciones 1, 2, 3 y 5 
    del documento auxiliar \emph{Suavizado de modelos de \(n\)-gramas}, \url{mod-prob-sec-n2.pdf}
  \end{block}
\end{frame}

\section{Modelos ocultos de Markov}
\begin{frame}
  \frametitle{El problema de la desambiguación léxica}
Usaremos el problema de la desambiguación léxica o \emph{part-of-speech tagging} para ilustrar los modelos ocultos de Markov.

[cont.]
\end{frame}


\end{document}













Hidden Markov models for part-of-speech tagging

The part-of-speech tagging (also “homograph-resolution”) problem: assign a category to each homograph in a sentence :

I {PRN}
saw {V,N}
the {DET}
book {V,N}
at {PRP}
home {N, ADV}

Of the 8 possible part-of-speech sequences:

PRN V DET V PRP N
PRN N DET V PRP N
PRN V DET N PRP N
PRN V DET V PRP N
PRN V DET V PRP ADV 
PRN N DET V PRP ADV
PRN V DET N PRP ADV
PRN V DET V PRP ADV

The one in bold should be the winner.

Sometimes the tags could be finer or coarser. For instance “training” could be just VBG (verb in “-ing”) or be classified as a gerund (“I was training” VB.GER), a present participle (“The training students”, VB.PAP) or a verbal noun (“The training was hard”, VB.N).

On the three ways of training a HMM for part-of-speech tagging:

Counting and smoothing on a tagged (disambiguated) corpus (SUPERVISED) GENERAL PURPOSE
Expectattion maximization (Baum-Welch alg.) on an untagged (ambiguous) corpus (UNSUPERVISED) GENERAL PURPOSEa
Using info from the target language (train the tagger “in place”)(UNSUPERVISED) (SPECIFIC PURPOSE=MT)
Blackboard picture:


Observables are word classes. An example:

I book the room
{PRN} {VB,NN} “the” {NN}

The problem is: how do we get the best tag sequence γ*[1] γ*[2] γ*[3] γ*[4]

Word classes Σ = { {PRN}, {VB, NN}, “the”, {NN}, ...}
Tags: Γ = { PRN, VB, “the” ...}

Note that Σ is the capital letter of σ and Γ is capital letter of γ (engineers should know all greek letters as they are widely used!).

Notation: Power set: P(A) = 2A  is the set of all subsets of A. For instance, If A={a,b,c}. P(A) = 2A ={0,{a},{b},{c},{a,b},{a,c},{b,c},{a,b,c}}

Bayes rule: p(A|B) = p(B)p(A)/p(B).

One of the main ideas of HMM applied to PoS is to separate the lexical and the syntactic modelling:	
	 	
p(γ[1]γ[2]...γ[L]|σ[1]σ[2]...σ[L])	x	p(γ[1]γ[2]... γ[L])
{		LEXCAL	 }		{ SYNTACTIC }




Both n-gram models and hidden Markov models have been widely used in another human language  application, namely, automated speech recognition. Some of the pioneers of statistical machine translation (such as Hermann Ney, and Frederik Jelinek) come from that area.
n-gram models and their estimation from a corpus
Smoothing to avoid the problem of zero probabilities when the n-gram model is used to calculate the probability of a new sequence: Blackboard explanation following the technical notes on “smoothing” delivered (separate PDF file), including the Brants and Samuelsson discount

Notes to support the explanation of smoothing: 
	 	 	
Smoothing uses clever approximations to avoid zeroes in probability expressions due to unseen events. It can be said to be “an art”, as there are many such approximations.

On the division of the corpus for deleted interpolation in three parts:
kept or training, where one counts 	C() and  f3,f2,f1
held-out, tuning, or development corpus, where one determines λ1 λ2 λ3
test corpus


Application of hidden Markov models for lexical categorical disambiguation (or part-of-speech tagging) / part 1

First approach to the problem (first slides in “Part-of-speech tagging using hidden Markov models” handout). Learning from a labeled corpus (supervised learning).

Definitions and fundamentals before explaining:
Surface form (SF): the form of the word as is found in the text (sang)
Lexical Form (LF): analysis of the word 
lemma (or dictionary form: sing), 
lexical category or part of speech (verb), and 
inflection indicators (past tense)
Homography: when a SF corresponds more than one LF: e.g.
work → work.vblex.pres.not3rd
work → work.vblex.inf
work → work.n.sg 
properly resolving the homography is critical in machine translation:
I work → Je travaille
I came to work → Je suis venu pour travailler
This is hard work →C’est un travail difficile.
Homography should not be confused with polysemy (a property of the word’s lemma, which can have more than one interpretation: the property is shared by all inflected forms)
To solve categorial lexical homography, one can use “coarse” tags such as as noun (n), or categories or “fine” categories such as “plural noun”. In some languages thee may even be intermediate categories. One usually tries to make those distinctions that are relevant to the application.
Use of ambiguity classes as observables. Ambiguous words can be grouped into  classes, which are essentially sets of tags, to reduce the number of model parameters. For example, the words work, place, book are all in the ambiguity class {vblex.pres.not3rd, vblex.inf, n.sg} and can be treated jointly.

Second approach to the problem (Last slides in “Part-of-speech tagging using hidden Markov models” handout): unsupervised learning from an untagged corpus. Further reading: article of Cutting et al. (1992) [2], which also explains the Baum-Welch algorithm [students read the article and try to understand as much as possible, to clear later their doubts with the instructor]


\end{document}


